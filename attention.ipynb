{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive for-loop average\n",
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n",
      "\n",
      "Matmul average\n",
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n",
      "\n",
      "Matmul softmax average\n",
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "# batch, time, channels\n",
    "# batch, input tokens, \"some information per each input token\"\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# we now want to somehow couple these 8 tokens, so that they can \n",
    "# share information between each other (without lookahead)\n",
    "\n",
    "# the easiest way to do that is to just average the previous tokens\n",
    "# of course it's also the most lossy way\n",
    "\n",
    "xbow = torch.zeros(B, T, C)\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        # we pick batch, and for each token in this batch\n",
    "        # gather all tokens leading up to it and token itself\n",
    "        xprev = x[b, :t+1]\n",
    "        xbow[b, t] = xprev.mean(0)\n",
    "\n",
    "print(\"Naive for-loop average\")\n",
    "print(xbow[0])\n",
    "\n",
    "# this can also be sped up using matmuls\n",
    "# we basically seed the multiplicative accumulators\n",
    "# and multiply the inputs with mean-making-matrix\n",
    "tril = torch.tril(torch.ones((8, 8)))\n",
    "tril = tril / tril.sum(1, keepdim=True)\n",
    "xbow2 = tril @ x\n",
    "\n",
    "print(\"\\nMatmul average\")\n",
    "print(xbow2[0])\n",
    "\n",
    "# the normalization of tril elements can also be done\n",
    "# using softmax, while filling 0 elements as -inf\n",
    "tril = torch.tril(torch.ones((8, 8)))\n",
    "mat = torch.zeros(8, 8)\n",
    "mat = mat.masked_fill(tril == 0, float(\"-inf\"))\n",
    "mat = mat.softmax(1)\n",
    "xbow3 = mat @ x\n",
    "\n",
    "print(\"\\nMatmul softmax average\")\n",
    "print(xbow2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5877, 0.4123, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4457, 0.2810, 0.2733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2220, 0.7496, 0.0175, 0.0109, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0379, 0.0124, 0.0412, 0.0630, 0.8454, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5497, 0.2187, 0.0185, 0.0239, 0.1831, 0.0062, 0.0000, 0.0000],\n",
       "        [0.2576, 0.0830, 0.0946, 0.0241, 0.1273, 0.3627, 0.0507, 0.0000],\n",
       "        [0.0499, 0.1052, 0.0302, 0.0281, 0.1980, 0.2657, 0.1755, 0.1474]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "# now that we know how to gather information, we can properly\n",
    "# get into how self-attention works\n",
    "# batch, tokens, embeddings\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# the idea is to build query and keys vectors\n",
    "# their dot product will build the average matrix\n",
    "# that we had before\n",
    "\n",
    "# we add linear layers that we will pass x through\n",
    "# so that the vectors can learn to behave like\n",
    "# queries and keys\n",
    "\n",
    "head_size = 16\n",
    "key   = nn.Linear(C, head_size, bias=False) # B, T, C -> B, T, 16\n",
    "query = nn.Linear(C, head_size, bias=False) # B, T, C -> B, T, 16\n",
    "value = nn.Linear(C, head_size, bias=False) # B, T, C -> B, T, 16\n",
    "\n",
    "k = key(x)\n",
    "q = query(x)\n",
    "\n",
    "# we now want a dot product to give us an attentiom matrix\n",
    "# as before, we want this matrix to be (B, T, T)\n",
    "# as both vectors are (B, T, 16), we need to transpose\n",
    "# so: (B, T, 16) x (B, 16, T) -> (B, T, T)\n",
    "# torch transpose is not ordered, only asks what dims\n",
    "\n",
    "mat = k @ q.transpose(-2, -1)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "mat = mat.masked_fill(tril == 0, float(\"-inf\"))\n",
    "mat = mat.softmax(-1)\n",
    "\n",
    "# softmax(qk) @ v \n",
    "\n",
    "v = value(x)\n",
    "att = mat @ v # [B, T, T] x [B, T, 16]\n",
    "\n",
    "# each row shows how interested the token equal to row id \n",
    "# was with each of tokens up to its position\n",
    "# ie. first row shows that first token was maximally interested\n",
    "# in token 1, becasue it cant see any future tokens, etc\n",
    "\n",
    "mat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q: Difference between encoder and decoder block\n",
    "# the main difference between a decoder and encoder blocks is \n",
    "# allowing a lookahead, by removing the tril masking\n",
    "# in an encoder all tokens see each other\n",
    "\n",
    "# Q: Self-attention vs cross-attention\n",
    "# self-attention means that q, k, v come from same source x\n",
    "# q, k can come from encoder blocks, while x comes\n",
    "# from decoder blocks -- then it is called cross attention\n",
    "\n",
    "# Q: Scaling by the sqrt(head_size)\n",
    "# scaling is again used to keep the variance in check\n",
    "\n",
    "# Q: Multihead attention\n",
    "# usually the head size is scaled by some x, and then additional x\n",
    "# attention heads are created, then you concatenate the outputs\n",
    "# and receive same shaped outputs, but heads can learn different \n",
    "# things"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modern_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
